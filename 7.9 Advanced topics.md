- - ## Advanced topics

    ### Unsupervised learning

    Unsupervised Learning methodologies are different from the Supervised Learning methodologies since they do not need any labelled data. Labels are vital for Supervised Learning, but there are situations in which generating or obtaining labelled data could be a problem. While doing academic research or implementing commercial software, you might encounter the following problems when dealing with a dataset:

    - No time (or no one) to label your data: EEG data processing requires lots of data. The neurologist labels the EEG data. Sometimes the neurologist is not present in the team or they need too much time to go through all the recordings.
    - Not enough data to label: Sometimes there is not enough data for the training set, the validation set and the test set. While developing commercial software, the client is generic and there could be little data available. It’s like buying shoes! You can get the pret-a-porter nice shoes or handmade Italian leather. Pret-a-porter shoes are comfortable and fit everyone. Handmade Italian leather shoes are more expensive and fit only one person, but they fit like a glove. Supervised Learning methods are the latter kind. To remove all the artifacts from the recordings you need to label all the specific person’s artifacts, which takes time and effort. Consumer methods work for everyone’s brain signals, but they don’t fit yours as well as the custom made ones.

    Unsupervised learning cuts out calibration, because they do not need labelled data. This reduces the time before online application. It also avoids problems due to wrong labelled data or to miscommunication between training instructor and user during the experiment.

    Unsupervised Learning methods are not only an alternative to Supervised methods. They are also a way to get information on a dataset in their own rights.

    Unsupervised Learning methods help with signal encoding/decoding and artifacts removal.

    The artifacts removal block in the pre-processing rejects the artifacts in the EEG. The most common artifacts are blinking, chewing, frowning and smiling. The EEG channels record the artifacts together with the actual signal and it is a priority to remove as much noise as we can before processing. See the [pre-processing lesson](http://learn.neurotechedu.com/preprocessing/) for more details.

    #### ICA

    The EEG signal has an extremely low SNR, is a non stationary signal and changes from person to person. It has no Gaussian distribution. For this reason **Independent Component Analysis** is applied: ICA calculates the directions of the subspace by maximizing their non-Gaussianity and separates independent sources, which have been recorded linearly mixed. We can only apply ICA to sources combined linearly. ICA tends to concentrate artifacts, thus it is a good method for artifacts removal. Applying ICA multiple times may improve the quality of the decomposition. In the R function there is the option to have the ica algorithm passed multiple times.

    In Independent Component Analysis, the directions have to be statistically independent. Let’s take into consideration the following latent model:

    ```
    **x** = A**s**
    ```

    where **x** is the initial n-dimensional dataset, where n in the number of EEG channels recording the signal. **s** is composed by l (with l < n ) mutually statistically independent latent variables, which are projected on l independent directions. In this case the signals from all the channels together form the dataset.

    The input matrix **x** represents the collection of the signals from all the channels: voltage difference between channels or between a channel and the reference (usually the electrode on the ear).

    ICA also works a priori and has no hypothesis regarding the order of the EEG signals. So you can swap the order of the channels around and the change doesn’t reflect on the processing. So when you input the matrix of the EEG channels you don’t need to worry about the cables order.

    The output matrix **s** contains the independent components, the mutually statistically independent latent variables. Therefore you can isolate domains of cortical synchrony.

    A is called mixing matrix.

    We aim to find the vector of lower dimension variables and the values of these variables in the new subspace. This subspace’s dimension will be lower than the one we previously had.

    ICA example by the MNE library:

    https://mne.tools/stable/auto_tutorials/discussions/plot_background_ica.html#sphx-glr-auto-tutorials-discussions-plot-background-ica-py

    Details on ICA Methods

    Infomax is a method based on maximizing the joint entropy of a nonlinear function of the estimated source signals. See Bell and Sejnowski (1995) and Helwig (in prep) for specifics of algorithms.

    FastICA is a method based on maximizing the negentropy of the estimated source signals. Negentropy is calculated based on the expectation of the contrast function. See Hyvarinen (1999) for specifics of fixed-point algorithm.

    JADE calculates the subspace by diagonalizing the cumulant array of the source signals. See Cardoso and Souloumiac (1993,1996) and Helwig and Hong (2013) for specifics of the JADE algorithm.

    Example:

    In the following example, 3 signals will be artificially created and mixed. Then, we will process the signals with ICA and see how it works.

    ##### Python

    There are many useful libraries in this case, depending on the level of coding you want to apply and implement. We are going to review MNE, which is specific for EEG.

    mne.processing.ICA class implements ICA with four different methods: fastica, picard, infomax, and the extended-infomax algorithm. Bad data segments can be rejected by reject parameter mne.preprocessing.ICA.fit.

    Source: https://mne.tools/stable/auto_tutorials/discussions/plot_background_ica.html#sphx-glr-auto-tutorials-discussions-plot-background-ica-py

    ```
    #Import required libraries for generating the sample signals
    import numpy as np
    import matplotlib.pyplot as plt
    from scipy import signal
    
    #Creating sample signals
    
    np.random.seed(101)
    size = 5000
    time = np.linspace(0, 8, size)
    
    signal1 = np.sin(2 * time)  
    signal2 = signal.sawtooth(2 * np.pi * time)
    signal3 = np.sign(np.sin(3 * time))  
    
    final_signal = np.vstack((signal1, signal2, signal3)).T
    final_signal += 0.2 * np.random.normal(size=final_signal.shape)  # Add noise
    
    # Mix signals data
    A = np.array([[1, 1, 1], [0.5, 2, 1.0], [1.5, 1.0, 2.0]])  # Mixing matrix
    mixed_signal = np.dot(final_signal, A.T)  # Generate observations
    
    
    #Importing libraries to process signals
    from sklearn.decomposition import FastICA, PCA
    
    #PCA processing
    pca = PCA(n_components = 3)
    pca_signal = pca.fit_transform(mixed_signal)
    
    #ICA processing
    ica = FastICA(n_components=3)
    ica_signal = ica.fit_transform(mixed_signal)
    
    
    #Ploting data
    
    fig, axs = plt.subplots(4,1, figsize=(10, 3*4))
    
    #1st: Raw signal
    
    ax = axs[0]
    ax.set_title('Real signals')
    ax.plot(np.arange(len(final_signal)), final_signal[:,0])
    ax.plot(np.arange(len(final_signal)), final_signal[:,1])
    ax.plot(np.arange(len(final_signal)), final_signal[:,2])
    
    #2nd: Mixed signal
    
    ax = axs[1]
    ax.set_title('Mixed signals')
    ax.plot(np.arange(len(mixed_signal)), mixed_signal[:,0])
    ax.plot(np.arange(len(mixed_signal)), mixed_signal[:,1])
    ax.plot(np.arange(len(mixed_signal)), mixed_signal[:,2])
    
    #3rd: PCA signal
    
    ax = axs[2]
    ax.set_title('PCA result')
    ax.plot(np.arange(len(pca_signal)), pca_signal[:,0])
    ax.plot(np.arange(len(pca_signal)), pca_signal[:,1])
    ax.plot(np.arange(len(pca_signal)), pca_signal[:,2])
    
    #4th: ICA signal
    
    ax = axs[3]
    ax.set_title('ICA results')
    ax.plot(np.arange(len(ica_signal)), ica_signal[:,0])
    ax.plot(np.arange(len(ica_signal)), ica_signal[:,1])
    ax.plot(np.arange(len(ica_signal)), ica_signal[:,2])
    
    plt.tight_layout()
    plt.show()
    ```

    ![Alt text](http://learn.neurotechedu.com/images/machinelearning/image11.png)

    As it is possible to observe from the image above, when having the mixed signal, it is very difficult to distinguish between each of the 3 components. When the PCA process is applied, it is possible to clearly discern the individual components. Nevertheless, they do not follow the same pattern as the real data. Finally, we can see why ICA is most commonly used on this type of problems, as each one of the components obtained has the same kind of signal profile as the initial data.

    ```
    # init ICA. Feed dataset, method and random_state
    
    ica =<span style="text-decoration:underline;"> </span>ICA(n_components=n_components, method=method, random_state=random_state)
    
    # define what kind of components you want to reject
    
    reject = dict(mag=5e-12, grad=4000e-13) \
    ica.fit(raw, picks=picks_meg, decim=decim, reject=reject) \
    ica.plot_components()
    
    # i.e., to plot component 0:
    ica.plot_properties(raw, picks=0)
    ```

    ##### R

    eegkit package implements ICA methods in R. Methods available are “imax”, “fast”, and “jade”

    ```
    ica =<span style="text-decoration:underline;"> </span>eegica(n_components = n_components(rows = channels, cols = time points), nc, center = TRUE, maxit = 100, tol = 1e-6, Rmat = diag(nc), type = c(“time”, “space”), method = c(“imax”, “fast”, “jade”)
    ```

    `nc` is the number of components to extract. If `center` is TRUE then the functions mean-centers the columns before decomposition. maxit is the maximal number of algorithm iteration allowed. tol is the convergence tolerance. Rmat Initial estimate of the nc-by-nc orthogonal rotation matrix.

    ##### MATLAB

    The all inclusive EEG toolkit in MATLAB is EEGLAB. The user-friendly method to apply ICA is to call the function pop_runica and select variables and dataset directly from the pop up window. At this point the only options in EEGLAB are [runica](https://sccn.ucsd.edu/wiki/Chapter_09:_Decomposing_Data_Using_ICA) - which uses infomax - and jade. To use the function fastica, you need to install the fastica toolbox. Let’s also mention the faster version of runica, called binica. Both give stable decomposition while processing hundreds of channels.

    ##### Details on ICA Methods

    Infomax is a method based on maximizing the joint entropy of a nonlinear function of the estimated source signals. See Bell and Sejnowski (1995) and Helwig (in prep) for specifics of algorithms.

    FastICA is a method based on maximizing the negentropy of the estimated source signals. Negentropy is calculated based on the expectation of the contrast function. See Hyvarinen (1999) for specifics of fixed-point algorithm.

    JADE calculates the subspace by diagonalizing the cumulant array of the source signals. See Cardoso and Souloumiac (1993,1996) and Helwig and Hong (2013) for specifics of the JADE algorithm.

    #### Clustering Component Analysis

    Clustering methods can further develop the independent component analysis. When confronted with a pool of subjects, multiple sessions or different protocols, it might be useful to do a cross analysis. In case of software development, clustering the independent component analysis results could be useful for checking the results in real time and finding out outliers.

    Clustering is a method in Unsupervised Learning which allows you to group together elements with some commonalities. Usually we tend to cluster elements that have similar values for a specific feature. To do so the best technique is k-mean. It is demonstrated that this is the most efficient method to cluster elements.

    ##### Python / R

    There are several steps in the independent component clustering process:

    1. Identify a set of epoched EEG datasets for performing clustering.
    2. [Optional] Specify the subject code and group, task condition, and session for each dataset.
    3. Identify components to cluster from each dataset.
    4. Specify and compute measures to use to cluster (pre-clustering).
    5. Perform clustering based on these measures.
    6. View the scalp maps, dipole models, and activity measures of the clusters.
    7. [Optional] Perform signal processing and statistical estimation on component clusters.

    ##### MATLAB

    EEGLAB v5.0b has a new structure called STUDY. It allows to store different subjects, sessions and so on. It also allows to compare features between subjects, sessions and protocols. The function that allows clustering component analysis is [ICACLUST](https://sccn.ucsd.edu/wiki/Independent_Component_Clustering_Example).

    ### Online vs Offline applications

    Depending on the industry and the task, you may find yourself processing either real-time or offline.

    Usually experimental procedures used during research oriented experiments work on data gathered from subjects screened. Such data is previously recorded and then used later on. So research and most of accademia related software processes data offline.

    On the other hand, when designing and developing software, real time could be one of the requirements of the algorithm. In this scenario, the software needs to be tested before reaching the consumer. Testing protocols require a dataset already labelled to check the results and compare them to the software output. Therefore in both examples there is a dataset available.

    The presence of the dataset doesn’t automatically mean “Supervised Learning”. Software using “Unsupervised Learning” methodologies might be tested on previously recorded dataset before being released. It could be a very short dataset! Or not yet labelled. But the combination of offline - real time allows other strategies to be actualized.

    ### Combining classifiers

    #### Bagging

    The idea of bagging comes from a classic example that occurs frequently in the real world.

    Given some phenomenon that produces data, say time delay between stimulus and ERP response, I might collect some data from my observations and you might collect some data from your observations. We then fit our models via regression to predict the time delay based on various factors (age, weight, etc). However, it may turn out that the data we collect is localized due to some unforeseen factors (perhaps you used right handed subjects and I used left handed ones). In this instance, it is highly likely for any models that we build to lean towards the low bias, high variance, overfitted side of the bias-variance trade off. In other words, we overfit to the data we have collected and thus our models do not generalize well to the general population.

    Having encountered this problem, one solution might be to average our model parameters such that we have a new model that incorporates both of our data sets. This is the idea of bagging: to overfit many low bias, high variance models, then average to reduce variance.

    In practice, bagging is often used in conjunction with nonparametric techniques that often have high variance, canonical decision trees (i.e. random forest) but also k-nn, etc. Given some dataset, we take many samples via bootstrapping, and model each sample in parallel, overfitting every sample. We then average all the models together, and in theory, this leads to the same bias (low by default) while reducing the variance roughly by a factor of 1/M (M= number of samples). This rarely happens in practice - see the Cornell lecture regarding stable learners for more details.

    #### Boosting

    Another common scenario may play out as follows.

    Knowing about bagging, I split my dataset into some subsets. I fit some initial submodels and average, then predict some initial results. However, the performance may still be mediocre. This could be a case where the model leans towards the high bias, low variance side of the bias-variance trade off. Our initial submodels are too simple and do not capture the characteristics of the data; the models are under fitted.

    One thing I might do to improve performance is to study some model fit indicators, for example, residuals, to investigate how to better tune the submodels. I then make adjustments to my original submodels based on the results of my investigations. To really put my submodels to the test, I select data which I know my previous model struggled with - the ‘hard’ data points - and use these as new subsets to train my new submodels on. I aggregate (weighted average of each submodel) and produce a new model which fits better than the original. I repeat this process as necessary, updating my weights to preserve the good base models. This is the idea of boosting: sequentially fitting better and better sets of base models based on the residuals of the previous trials.

    In practice, boosting has proven effective in reducing bias and improving predictive power. It was a very popular option in contests due to its ease of implementation and its flexibility, to the point where there was a period of time when many competitions were only won by boosting algorithms. However, since boosting requires the sequential fitting of models as well needing to choose the ‘hard’ data points, it is computationally expensive. XGBoost is a popular gradient boosting tree package in Python.

    #### Stacking

    Until now, we have supposed that each of the base models described in the above sections were uniform (i.e. if we are bagging, the models we average are all decision trees). However, this need not be the case. Stacking refers to the technique whereby many different models are combined. In this case, a meta layer must be added to introduce a ‘reward’ mechanism for good models and a ‘punishment’ for poor models. This meta layer contains weights for each of the base models. Each base model predicts a value for the final outcome and then the values are multiplied by weights, then combined (usually via a logistic function) into a final result. The weights as well as the base models parameters are then adjusted based on feedback (e.g. stochastic gradient descent) and the theory is that eventually, the poor models are weeded out and only the powerful ones remain. Stacking can be thought of as boosting different models, but a good approach to tune the meta layer as well as propagate the errors back to the base models makes stacking much more difficult to implement in practice.